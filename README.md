
# ğŸ§± ClayBot - AI and ML Project for Shopify Support

Welcome to **ClayBot**, a smart, evolving chatbot built for **Clay Imports**. It boosts customer service in Shopify with semantic search, weekly training, real-time scraping, and OpenAI integration.  
> ğŸ§ª Currently under development â€“ **not live in production**.

---

## ğŸ“¦ General Project Structure

| Category | Files | Description |
|----------|----------|-------------|
| ğŸ¤– Core Bot | `server.py`, `bot.py` | Main backend of the chatbot |
| ğŸ§  Intent ML | `weekly_learning.py`, `check_duplicates.py`, `intent_model.joblib`, `training_data.json` | Intent classifier with weekly learning |
| ğŸ§± Collections/Products | `export_collections_and_products.py`, `generate_collection_descriptions.py`, `regenerate_cache.py`, `products.json`, `collections_described.json`, `cached_collections.joblib` | Extraction and enrichment of collections with OpenAI |
| ğŸ“„ Informational Pages | `utils.py`, `pages.json` | Downloading and caching help pages from Shopify |
| ğŸ“„ FAQS | `faq_search.py`, `faq_embeddings.pt`, `generate_faq_embeddings.py` | Uses FAQS.json, semantic search and GPT 3.5 turbo to deliver best responses  |
| ğŸ“° Blog | `build_articles.py`, `articles.json` | Downloading and caching Shopify blog posts |
| ğŸ” Page Matching | `page_scraper.py`, `smart_page_router.py` | Search, scrape, and summarize help pages by intent |
| âš™ï¸ Automation | `run_pipeline.py` |Runs the entire training, export, and update flow |
| ğŸ” Access | `google_credentials.json` | Logging in Google Sheets |
| ğŸ§¹ Utilities | `.gitignore`, `cleanup_vscode.sh` | Environment tools (optional) |

---

## âœ… Recommended Flow (Manual or Automated)

### ğŸ” Option A: Automated
```bash
python3 run_pipeline.py
```
This command runs in order:
1. ğŸ§  Retrain intention (`weekly_learning.py`)
2. ğŸ” Verifies duplicates (`check_duplicates.py`)
3. ğŸ§± Exports data (`export_collections_and_products.py`)
4. ğŸ§  Generates AI descriptions (`generate_collection_descriptions.py`)
5. ğŸ’¾ Regenerate bot cache (`regenerate_cache.py`)
6. ğŸ“° Updates blog articles (`build_articles.py`)

---

### ğŸ§ª Option B: Manual

```bash
python3 weekly_learning.py
python3 check_duplicates.py
python3 export_collections_and_products.py
python3 generate_collection_descriptions.py
python3 regenerate_cache.py
python3 build_articles.py
```

---

## ğŸ§  Page matching and scraping

ClayBot uses `page_scraper.py` and `smart_page_router.py` as internal modules to:

- Scrape real content from Shopify pages (`/pages/...`).
- Clean up HTML and irrelevant content.
- Summarizing content with OpenAI using `text-embedding-3-small` and `gpt-4o-mini`.
- Display the summary to the user with a link.

This happens automatically when an intent like `contact`, `shipping`, `our_story` or `search_pages` is detected.

ğŸ”’ You don't need to run these scripts manually. They're already integrated into the workflow `server.py`.

---

## ğŸ“ Generated Key Files

- `collections.json` â†’ export from Shopify
- `products.json` â†’ active products
- `collections_described.json` â†’ enriched collections
- `cached_collections.joblib` â†’ bot cache
- `intent_model.joblib` â†’ updated classifier
- `articles.json`, `pages.json` â†’ useful cached content

---

## âš ï¸ IMPORTANT NOTES

- **Do not delete `cached_collections.joblib`** unless you regenerate it.
- **Check `google_credentials.json` and your environment variables before running.**
- `utils.py` updates `pages.json` automatically from `server.py`.
- No need to run `page_scraper.py` manually. It is connected to `search_shopify_pages()`.

---

## ğŸ§  What's next?

- Improve `collection_embeddings.json` for semantic matching.
- Improve `run_pipeline.py` with conditional logic.

---

Made with â¤ï¸ for Clay Imports.
